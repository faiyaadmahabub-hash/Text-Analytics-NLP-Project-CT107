{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e352f6a3",
   "metadata": {},
   "source": [
    "# Q3 - Afanasev Artur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba0495c",
   "metadata": {},
   "source": [
    "## Dataset: Data_2.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c5f383",
   "metadata": {},
   "source": [
    "1. Demonstrate 3 POS tagging methods\n",
    "2. Explain differences \n",
    "3. Draw parse tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e391e217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for q3 we're using data_2 \n",
    "with open(\"../Data/Data_2.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    inputData = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e11da6",
   "metadata": {},
   "source": [
    "Testing if it was opened properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "befb3925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The big black dog barked at the white cat and chased away.\n"
     ]
    }
   ],
   "source": [
    "print(inputData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb47ee2",
   "metadata": {},
   "source": [
    "1) 3 pos tagging methods demonstration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "27bbf639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/afanasevartur/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/afanasevartur/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imports \n",
    "import nltk \n",
    "from textblob import TextBlob\n",
    "from nltk import pos_tag, word_tokenize, RegexpTagger\n",
    "\n",
    "#launch\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "64d45c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'big', 'black', 'dog', 'barked', 'at', 'the', 'white', 'cat', 'and', 'chased', 'away', '.']\n"
     ]
    }
   ],
   "source": [
    "#tokenization\n",
    "\n",
    "inputData_tokenized = word_tokenize(inputData)\n",
    "print(inputData_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2622d775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. NLTK POS Tagger Output:\n",
      "[('The', 'DT'), ('big', 'JJ'), ('black', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('white', 'JJ'), ('cat', 'NN'), ('and', 'CC'), ('chased', 'VBD'), ('away', 'RB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# 1. NLTK POS Tagger\n",
    "print(\"1. NLTK POS Tagger Output:\")\n",
    "nltk_tags = pos_tag(inputData_tokenized)\n",
    "print(nltk_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8e583fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. TextBlob POS Tagger Output:\n",
      "[('The', 'DT'), ('big', 'JJ'), ('black', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('white', 'JJ'), ('cat', 'NN'), ('and', 'CC'), ('chased', 'VBD'), ('away', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "# 2. TextBlob POS Tagger\n",
    "print(\"2. TextBlob POS Tagger Output:\")\n",
    "blob = TextBlob(inputData)\n",
    "textblob_tags = blob.tags\n",
    "print(textblob_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dbc9afba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Regular Expression Tagger Output:\n",
      "[('The', 'DT'), ('big', 'JJ'), ('black', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('white', 'JJ'), ('cat', 'NN'), ('and', 'CC'), ('chased', 'VBD'), ('away', 'RB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    " # 3. Regular Expression Tagger\n",
    "patterns = [\n",
    "    (r'(?i)^(the|a|an)$', 'DT'), # Determiners/articles (The, the)\n",
    "    (r'.*ed$', 'VBD'), # Past tense verbs (barked, chased)\n",
    "    (r'^(at|in|on|by)$', 'IN'), # Prepositions (at)\n",
    "    (r'^(and|but|or)$', 'CC'), # Conjunctions (and)\n",
    "    (r'^(away|here|there)$', 'RB'), # Adverbs (away)\n",
    "    (r'^(big|black|white)$', 'JJ'), # Adjectives (big, black, white)\n",
    "    (r'^\\.$', '.'), # Period at the end of a sentence\n",
    "    (r'.*', 'NN') # By default, everything else is considered a noun (dog, cat)\n",
    "    ]\n",
    "print(\"3. Regular Expression Tagger Output:\")\n",
    "regexp_tagger = RegexpTagger(patterns)\n",
    "regexp_tags = regexp_tagger.tag(inputData_tokenized)\n",
    "print(regexp_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78354041",
   "metadata": {},
   "source": [
    "2) Differences Explanation\n",
    "\n",
    "The most important obvious difference between these 3 methods is that NLTK and TextBlob rely on statistical machine learning models trained on huge corpora of text, while Regular Expression (Regex) Tagger is a strictly deterministic algorithm based on human-written rules.\n",
    "\n",
    "breakdown by criteria:\n",
    "\n",
    "Accuracy - \n",
    "NLTK and TextBlob: Highly accurate. They analyze the context of a sentence. For the phrase \"The big black dog barked...,\" they will accurately identify \"barked\" as a past tense verb (VBD) and \"dog\" as a noun (NN), based on the probability distribution of words around them.\n",
    "Regex Tagger: Has extremely low accuracy on general text. Its accuracy is 100% only for words that strictly match the specified regular expressions, and 0% for all others. It doesn't understand context (for example, it won't distinguish the word \"book\" as a book from \"book\" as a reservation, unless a strict rule is specified).\n",
    "\n",
    "Tag Sets - \n",
    "NLTK and TextBlob: Out of the box, they use the standardized Penn Treebank tag set (e.g., NN for nouns, JJ for adjectives, VBD for past tense verbs). They understand and apply dozens of different tags for fine-grained classification.\n",
    "Regex Tagger: The tag set is entirely up to the programmer. The developer must manually specify the pattern conformance to the Penn Treebank standard (as we did in the code: .*ed$ -> VBD). Any omission will result in incorrect tagging (for example, irregular verbs like \"slept\" will not match the .*ed$ rule).\n",
    "\n",
    "Performance - \n",
    "Regex Tagger: Wins in pure computation speed at the micro-level. String matching using regular expressions is lightning fast and doesn't require loading heavy models into RAM. However, creating a complex system of rules for the entire English language would take years of manual effort.\n",
    "NLTK and TextBlob: They require time for initialization (loading pre-trained models, such as averaged_perceptron_tagger ) and consume more RAM. However, on the scale of real-world problems, their efficiency is incomparably higher, as they process any text without the need to write new rules.\n",
    "\n",
    "Use Cases - \n",
    "NLTK: The industry standard for academic and complex NLP tasks. Used when full control over the natural language processing pipeline and high accuracy are required.\n",
    "TextBlob: Ideal for rapid prototyping. It's a wrapper around NLTK, providing a simpler and more intuitive API. Suitable for basic analytics tasks where you need to write code quickly and without unnecessary configuration.\n",
    "Regex Tagger: Absolutely unsuitable as a standalone tagger for general-purpose text. However, it is indispensable in two cases:\n",
    "As a backoff tagger when the statistical model fails to handle an unknown word.\n",
    "For highly specialized subject areas (e.g., extracting and tagging specific part numbers, phone numbers, or currency codes with a rigid structure)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4fa553",
   "metadata": {},
   "source": [
    "3) Parse Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ab3c43c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse Trees:\n",
      "                              S                                        \n",
      "      ________________________|_____________________________________    \n",
      "     |                                 VP                           |  \n",
      "     |                         ________|___________________         |   \n",
      "     |                       VP1                |          |        |  \n",
      "     |               _________|___              |          |        |   \n",
      "     |              |             PP            |          |        |  \n",
      "     |              |      _______|____         |          |        |   \n",
      "     NP             |     |            NP       |         VP2       |  \n",
      "  ___|_________     |     |    ________|____    |     _____|___     |   \n",
      "Det Adj  Adj   N    V     P  Det      Adj   N   CC   V        Adv  Punc\n",
      " |   |    |    |    |     |   |        |    |   |    |         |    |   \n",
      "The big black dog barked  at the     white cat and chased     away  .  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk import CFG, ChartParser\n",
    "\n",
    "# define CFG\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "  S -> NP VP Punc\n",
    "  NP -> Det Adj Adj N | Det Adj N\n",
    "  VP -> VP1 CC VP2\n",
    "  VP1 -> V PP\n",
    "  VP2 -> V Adv\n",
    "  PP -> P NP\n",
    "  \n",
    "  Det -> 'The' | 'the'\n",
    "  Adj -> 'big' | 'black' | 'white'\n",
    "  N -> 'dog' | 'cat'\n",
    "  V -> 'barked' | 'chased'\n",
    "  P -> 'at'\n",
    "  CC -> 'and'\n",
    "  Adv -> 'away'\n",
    "  Punc -> '.'\n",
    "\"\"\")\n",
    "\n",
    "parser = ChartParser(grammar)\n",
    "\n",
    "#showcase\n",
    "print(\"Parse Trees:\")\n",
    "for tree in parser.parse(inputData_tokenized):\n",
    "    tree.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd4d2d1",
   "metadata": {},
   "source": [
    "# Q5 - Alternative approach: Spacy - Afanasev Artur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22655475",
   "metadata": {},
   "source": [
    "## Dataset - Data_1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814d9239",
   "metadata": {},
   "source": [
    "## code implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a4ab7c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens using spaCy:\n",
      "\n",
      "['Classification', 'is', 'the', 'task', 'of', 'choosing', 'the', 'correct', 'class', 'label', 'for', 'a', 'given', 'input', '.', 'In', 'basic', '\\n', 'classification', 'tasks', ',', 'each', 'input', 'is', 'considered', 'in', 'isolation', 'from', 'all', 'other', 'inputs', ',', 'and', 'the', 'set', 'of', 'labels', 'is', 'defined', 'in', 'advance', '.', 'The', 'basic', 'classification', 'task', 'has', 'a', 'number', 'of', 'interesting', 'variants', '.', 'For', 'example', ',', 'in', 'multiclass', 'classification', ',', 'each', 'instance', 'may', 'be', 'assigned', 'multiple', 'labels', ';', 'in', 'open', '-', 'class', 'classification', ',', 'the', 'set', 'of', 'labels', 'is', 'not', 'defined', 'in', 'advance', ';', 'and', 'in', 'sequence', 'classification', ',', 'a', 'list', 'of', 'inputs', 'are', 'jointly', 'classified', '.']\n",
      "\n",
      "Processing time: 0.01418 seconds\n"
     ]
    }
   ],
   "source": [
    "#install spacy and download the english model\n",
    "#pip3 install spacy\n",
    "\n",
    "import spacy\n",
    "import time\n",
    "\n",
    "#load the text file (assuming Data_1.txt is in the Data folder)\n",
    "with open(\"../Data/Data_1.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#text process\n",
    "start_time = time.time()\n",
    "doc = nlp(text)\n",
    "spacy_time = time.time() - start_time\n",
    "\n",
    "#extract text from spaCy Token objects\n",
    "spacy_tokens = [token.text for token in doc]\n",
    "\n",
    "print(\"Tokens using spaCy:\\n\")\n",
    "print(spacy_tokens)\n",
    "print(f\"\\nProcessing time: {spacy_time:.5f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24b1cf3",
   "metadata": {},
   "source": [
    "## comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e1a859e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Group's Approach (NLTK) ---\n",
      "Total tokens: 94\n",
      "Tokens with punctuation attached: 13\n",
      "Hyphenated words split incorrectly: 0\n",
      "Unique tokens: 53\n",
      "\n",
      "--- Alternative Approach (spaCy) ---\n",
      "Total tokens: 97\n",
      "Tokens with punctuation attached: 13\n",
      "Hyphenated words split incorrectly: 1\n",
      "Unique tokens: 55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def analyze_token_quality(method_title, token_list, raw_string):\n",
    "    # 1. count all and unique tokens \n",
    "    token_count = len(token_list)\n",
    "    vocab_size = len(set(token_list))\n",
    "    \n",
    "    # 2. checks punctuations mistakes\n",
    "    punct_chars = set(string.punctuation)\n",
    "    attached_punct = 0\n",
    "    for token in token_list:\n",
    "        if punct_chars.intersection(token):\n",
    "            attached_punct += 1\n",
    "            \n",
    "    # 3.check hyphen\n",
    "    raw_words = raw_string.split()\n",
    "    words_with_hyphens = [w for w in raw_words if '-' in w]\n",
    "    \n",
    "    broken_hyphen_count = 0\n",
    "    for hyphen_word in words_with_hyphens:\n",
    "        if hyphen_word not in token_list:\n",
    "            broken_hyphen_count += 1\n",
    "            \n",
    "    print(f\"--- {method_title} ---\")\n",
    "    print(\"Total tokens:\", token_count)\n",
    "    print(\"Tokens with punctuation attached:\", attached_punct)\n",
    "    print(\"Hyphenated words split incorrectly:\", broken_hyphen_count)\n",
    "    print(\"Unique tokens:\", vocab_size)\n",
    "    print()\n",
    "\n",
    "nltk_result = word_tokenize(text)\n",
    "\n",
    "analyze_token_quality(\"Group's Approach (NLTK)\", nltk_result, text)\n",
    "analyze_token_quality(\"Alternative Approach (spaCy)\", spacy_tokens, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9be7958",
   "metadata": {},
   "source": [
    "## 5.2 Compare and contrast the alternative approach with the groupâ€™s approach.\n",
    "\n",
    "The alternative approach implemented is the spaCy tokenizer which is contrasts significantly with the group's approaches in its underlying architecture and data representation.\n",
    "\n",
    "1. Object-Oriented vs. String-Based: The group's tokenizers like nltk or regEx primarily return an array of plain strings. In contrast spaCy returns a `Doc` object containing `Token` objects. Each token is not just a string, but a rich object that already holds linguistic annotations such as like part-of-speech tags, lemmas, and whether it is a stop word.\n",
    "2. Non-Destructive Tokenization: regEx and etc often lose original whitespace characters. NLTK separates punctuation but reconstructing the exact original text can be tricky. spaCy is completely non-destructive; it preserves all original whitespaces internally, allowing us to perfectly reconstruct the original text at any time.\n",
    "3. Contextual Awareness: While NLTK uses a generalized statistical model (`punkt`) spaCy uses a modern neural-network-backed pipeline (`en_core_web_sm`) which is highly optimized for complex, modern text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a3f88c",
   "metadata": {},
   "source": [
    "## 5.3 Explain why the alternative approach is better, worse, or just different.\n",
    "\n",
    "The spaCy approach serves a fundamentally different purpose compared to traditional NLP tokenizers.\n",
    "\n",
    "Reasons why spaCy is BETTER:\n",
    "Production-Ready and Fast: While it takes slightly longer to load the model initially, spaCy is written in Cython and is heavily optimized for speed on massive datasets compared to pure Python implementations like NLTK.\n",
    "Rich Linguistic Features: Tokenizing with spaCy automatically computes POS tags, syntactic dependencies, and named entities in one go, making it immensely powerful for complex text analytics tasks.\n",
    "Intelligent Edge-Case Handling: It handles complex punctuation, currency symbols, and abbreviations exceptionally well without requiring custom RegEx rules.\n",
    "\n",
    "Reasons why spaCy is WORSE:\n",
    "Resource Intensive: It requires downloading and loading heavy language models (e.g., `en_core_web_sm`), consuming significantly more RAM than simple NLTK or RegEx operations.\n",
    "Less Customizable: NLTK allows developers to easily build custom tokenizers from scratch. spaCy is a \"black box\" pipeline; changing its core tokenization behavior for highly niche, non-standard text like obscure programming code is much more difficult.\n",
    "\n",
    "Final conclusion:\n",
    "The spaCy approach is better for real-world application development where speed on large corpora and rich linguistic metadata are required. However, the group's approach (especially NLTK) remains better for academic environments, rapid prototyping, and scenarios where memory resources are strictly limited."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
