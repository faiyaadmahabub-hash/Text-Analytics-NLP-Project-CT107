{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e39fa41f",
   "metadata": {},
   "source": [
    "# Part A (Q1) - SALSABILA AURELIA OKTANIOPUTRI\n",
    "- Tokenization, Stop Words & Punctuation \n",
    "- Dataset: Data_1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043d7f33",
   "metadata": {},
   "source": [
    "## Load Data_1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0b5698c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification is the task of choosing the correct class label for a given input. In basic\n",
      "classification tasks, each input is considered in isolation from all other inputs, and the set of labels is defined in advance. The basic classification task has a number of interesting variants. For example, in multiclass classification, each instance may be assigned multiple labels; in open-class classification, the set of labels is not defined in advance; and in sequence classification, a list of inputs are jointly classified.\n"
     ]
    }
   ],
   "source": [
    "# load the text file\n",
    "with open(\"../Data/Data_1.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1f08a6",
   "metadata": {},
   "source": [
    "## 1. Demonstrate word tokenization using the split function, Regular expression and NLTK packages separately and report the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4037b57f",
   "metadata": {},
   "source": [
    "### 1.1. Tokenization Using split()\n",
    "\n",
    "The split() function separates text based on whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22cdc0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens using split():\n",
      "\n",
      "['Classification', 'is', 'the', 'task', 'of', 'choosing', 'the', 'correct', 'class', 'label', 'for', 'a', 'given', 'input.', 'In', 'basic', 'classification', 'tasks,', 'each', 'input', 'is', 'considered', 'in', 'isolation', 'from', 'all', 'other', 'inputs,', 'and', 'the', 'set', 'of', 'labels', 'is', 'defined', 'in', 'advance.', 'The', 'basic', 'classification', 'task', 'has', 'a', 'number', 'of', 'interesting', 'variants.', 'For', 'example,', 'in', 'multiclass', 'classification,', 'each', 'instance', 'may', 'be', 'assigned', 'multiple', 'labels;', 'in', 'open-class', 'classification,', 'the', 'set', 'of', 'labels', 'is', 'not', 'defined', 'in', 'advance;', 'and', 'in', 'sequence', 'classification,', 'a', 'list', 'of', 'inputs', 'are', 'jointly', 'classified.']\n"
     ]
    }
   ],
   "source": [
    "split_tokens = text.split()\n",
    "print(\"Tokens using split():\\n\")\n",
    "print(split_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d9bfda",
   "metadata": {},
   "source": [
    "### 1.2. Tokenization Using Regular Expression\n",
    "RegEx extract only word characters and remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f39f2403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens using Regular Expression:\n",
      "\n",
      "['Classification', 'is', 'the', 'task', 'of', 'choosing', 'the', 'correct', 'class', 'label', 'for', 'a', 'given', 'input', 'In', 'basic', 'classification', 'tasks', 'each', 'input', 'is', 'considered', 'in', 'isolation', 'from', 'all', 'other', 'inputs', 'and', 'the', 'set', 'of', 'labels', 'is', 'defined', 'in', 'advance', 'The', 'basic', 'classification', 'task', 'has', 'a', 'number', 'of', 'interesting', 'variants', 'For', 'example', 'in', 'multiclass', 'classification', 'each', 'instance', 'may', 'be', 'assigned', 'multiple', 'labels', 'in', 'open', 'class', 'classification', 'the', 'set', 'of', 'labels', 'is', 'not', 'defined', 'in', 'advance', 'and', 'in', 'sequence', 'classification', 'a', 'list', 'of', 'inputs', 'are', 'jointly', 'classified']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "regex_tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "print(\"Tokens using Regular Expression:\\n\")\n",
    "print(regex_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b0e45a",
   "metadata": {},
   "source": [
    "### 1.3. Tokenization Using Regular Expression\n",
    "\n",
    "NLTK word_tokenize() handles punctuation more linguistically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f969d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe\n"
     ]
    }
   ],
   "source": [
    "# debugging\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a69c5eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\asus/nltk_data', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\\\nltk_data', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\\\share\\\\nltk_data', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\asus\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    }
   ],
   "source": [
    "# debugging\n",
    "import nltk\n",
    "print(nltk.data.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02c3ea6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d2fca952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens using NLTK: \n",
      "\n",
      "['Classification', 'is', 'the', 'task', 'of', 'choosing', 'the', 'correct', 'class', 'label', 'for', 'a', 'given', 'input', '.', 'In', 'basic', 'classification', 'tasks', ',', 'each', 'input', 'is', 'considered', 'in', 'isolation', 'from', 'all', 'other', 'inputs', ',', 'and', 'the', 'set', 'of', 'labels', 'is', 'defined', 'in', 'advance', '.', 'The', 'basic', 'classification', 'task', 'has', 'a', 'number', 'of', 'interesting', 'variants', '.', 'For', 'example', ',', 'in', 'multiclass', 'classification', ',', 'each', 'instance', 'may', 'be', 'assigned', 'multiple', 'labels', ';', 'in', 'open-class', 'classification', ',', 'the', 'set', 'of', 'labels', 'is', 'not', 'defined', 'in', 'advance', ';', 'and', 'in', 'sequence', 'classification', ',', 'a', 'list', 'of', 'inputs', 'are', 'jointly', 'classified', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk_tokens = word_tokenize(text)\n",
    "\n",
    "print(\"Tokens using NLTK: \\n\")\n",
    "print(nltk_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bf578b",
   "metadata": {},
   "source": [
    "### 2.\tJustify the most suitable tokenisation operation for text analytics. Support your answer using obtained outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f161fd67",
   "metadata": {},
   "source": [
    "Below is a function that can calculate the metrics of the results of each approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2619d5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Split() ---\n",
      "Total tokens: 82\n",
      "Tokens with punctuation attached: 13\n",
      "Hyphenated words split incorrectly: 0\n",
      "Unique tokens: 55\n",
      "\n",
      "--- Regex ---\n",
      "Total tokens: 83\n",
      "Tokens with punctuation attached: 0\n",
      "Hyphenated words split incorrectly: 1\n",
      "Unique tokens: 50\n",
      "\n",
      "--- NLTK ---\n",
      "Total tokens: 94\n",
      "Tokens with punctuation attached: 13\n",
      "Hyphenated words split incorrectly: 0\n",
      "Unique tokens: 53\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def tokenizer_metrics(name, tokens, original_text):\n",
    "    total = len(tokens)\n",
    "    # Count tokens with punctuation attached (like input., tasks,)\n",
    "    punct_attached = sum(1 for t in tokens if any(p in t for p in string.punctuation))\n",
    "    # Unique tokens (vocab size)\n",
    "    unique = len(set(tokens))\n",
    "    \n",
    "    # Count hyphenated words in original text that were split incorrectly\n",
    "    hyphen_words = [word for word in original_text.split() if '-' in word]\n",
    "    hyphen_split_count = sum(1 for hw in hyphen_words if hw not in tokens)\n",
    "    \n",
    "    print(f\"--- {name} ---\")\n",
    "    print(\"Total tokens:\", total)\n",
    "    print(\"Tokens with punctuation attached:\", punct_attached)\n",
    "    print(\"Hyphenated words split incorrectly:\", hyphen_split_count)\n",
    "    print(\"Unique tokens:\", unique)\n",
    "    print()\n",
    "\n",
    "# Compare Tokenizers\n",
    "tokenizer_metrics(\"Split()\", split_tokens, text)\n",
    "tokenizer_metrics(\"Regex\", regex_tokens, text)\n",
    "tokenizer_metrics(\"NLTK\", nltk_tokens, text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4018d4a0",
   "metadata": {},
   "source": [
    "Three tokenization methods are evaluated: **Split()**, **Regular Expression (Regex)**, and **NLTK's `word_tokenize()`**.  \n",
    "The evaluation metrics are:  \n",
    "\n",
    "- **Total Tokens** – number of tokens generated  \n",
    "- **Tokens with Punctuation Attached** – words with punctuation still stuck (`input.`, `tasks,`)  \n",
    "- **Unique Tokens** – vocabulary size  \n",
    "- **Hyphenated Words Split Incorrectly** – counts hyphenated words like `open-class` that got split incorrectly  \n",
    "\n",
    "**1. split()**\n",
    "Metrics from the results: \n",
    "  - Total tokens: 82  \n",
    "  - Tokens w/ punctuation attached: 13  \n",
    "  - Unique tokens: 55  \n",
    "  - Hyphenated words split incorrectly: 0  \n",
    "\n",
    "Analysis & Proof:\n",
    "- Punctuation is not separated. 13 tokens have punctuation still attached. Example: `input.` and `tasks,`. Due to punctuation not being separated, vocabulary size is inflated thus introduces noise. \n",
    "- Hyphenated words are preserved because punctuation stays attached (`open-class` counted as one token).  \n",
    "- Limitation: Preserving punctuation with words is not suitable for ML models, due to the fact that it will unnecessarily increases feature dimensionality.\n",
    "\n",
    "\n",
    "**2. Regular Expression**\n",
    "Metrics from the results:\n",
    "  - Total tokens: 83  \n",
    "  - Tokens w/ punctuation attached: 0  \n",
    "  - Unique tokens: 50  \n",
    "  - Hyphenated words split incorrectly: 1  \n",
    "\n",
    "Analysis & Proof:\n",
    "- Punctuation is completely removed, 0 punctuation-attached tokens.  \n",
    "- Example: `input.` → `input`, `tasks,` → `tasks`.  \n",
    "- Limitation: Hyphenated word `open-class` is split into `open` and `class` thus losing the semantic meaning.  \n",
    "- Vocabulary slightly smaller (50 unique tokens) due to punctuation removal.\n",
    "- RegEX offers cleaner tokens but loses some of the semantic integrity for hyphenated words.  \n",
    "\n",
    "**3. NLTK**\n",
    "Metrics from the results:\n",
    "  - Total tokens: 94  \n",
    "  - Tokens w/ punctuation attached: 13  \n",
    "  - Unique tokens: 53  \n",
    "  - Hyphenated words split incorrectly: 0  \n",
    "\n",
    "Analysis & Proof:\n",
    "- Punctuation is separated as individual tokens. Example from first 15 tokens: ['input', '.', 'tasks', ',', 'labels', ';', 'open-class']\n",
    "- The period (`.`), comma (`,`), and semicolon (`;`) are counted as separate tokens.  \n",
    "- Hyphenated words (`open-class`) are preserved which offers correct semantic representation.  \n",
    "- Total token count is higher because punctuation is separated, but this allows controlled preprocessing (e.g., removing punctuation or stopwords).  \n",
    "- Vocabulary size (53 unique tokens) is appropriate — balances completeness and accuracy.\n",
    "\n",
    "**Conclusion**\n",
    "- **Split()**: Keeps punctuation attached however it will inflate tokens which is not ideal for ML preprocessing.  \n",
    "- **Regex**: Clean punctuation which is good for counting, but splits hyphenated words thus loses semantic meaning.  \n",
    "- **NLTK**: Linguistically aware but separates punctuation, preserves hyphenated words. NLTK is the  **best choice** for downstream text analytics tasks.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55364f9e",
   "metadata": {},
   "source": [
    "answer it here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6374ea7f",
   "metadata": {},
   "source": [
    "### 3.\tDemonstrate stop words and punctuations removal and report the output suitably along with the stop words found in the given text corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca176d1",
   "metadata": {},
   "source": [
    "### 3.1. Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb6122af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Words Found in Corpus:\n",
      "\n",
      "{'other', 'not', 'is', 'all', 'the', 'and', 'each', 'in', 'has', 'are', 'from', 'for', 'of', 'a', 'be'}\n",
      "\n",
      "Tokens after Stop Word Removal:\n",
      "\n",
      "['classification', 'task', 'choosing', 'correct', 'class', 'label', 'given', 'input', '.', 'basic', 'classification', 'tasks', ',', 'input', 'considered', 'isolation', 'inputs', ',', 'set', 'labels', 'defined', 'advance', '.', 'basic', 'classification', 'task', 'number', 'interesting', 'variants', '.', 'example', ',', 'multiclass', 'classification', ',', 'instance', 'may', 'assigned', 'multiple', 'labels', ';', 'open-class', 'classification', ',', 'set', 'labels', 'defined', 'advance', ';', 'sequence', 'classification', ',', 'list', 'inputs', 'jointly', 'classified', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Lowercase tokens\n",
    "tokens_lower = [word.lower() for word in nltk_tokens]\n",
    "\n",
    "# Remove stopwords\n",
    "filtered_tokens = [word for word in tokens_lower if word not in stop_words]\n",
    "\n",
    "print(\"Stop Words Found in Corpus:\\n\")\n",
    "found_stopwords = [word for word in tokens_lower if word in stop_words]\n",
    "print(set(found_stopwords))\n",
    "\n",
    "print(\"\\nTokens after Stop Word Removal:\\n\")\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ce224e",
   "metadata": {},
   "source": [
    "### 3.2. Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "89fc1c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Tokens after Removing Stop Words and Punctuation:\n",
      "\n",
      "['classification', 'task', 'choosing', 'correct', 'class', 'label', 'given', 'input', 'basic', 'classification', 'tasks', 'input', 'considered', 'isolation', 'inputs', 'set', 'labels', 'defined', 'advance', 'basic', 'classification', 'task', 'number', 'interesting', 'variants', 'example', 'multiclass', 'classification', 'instance', 'may', 'assigned', 'multiple', 'labels', 'open-class', 'classification', 'set', 'labels', 'defined', 'advance', 'sequence', 'classification', 'list', 'inputs', 'jointly', 'classified']\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "final_tokens = [word for word in filtered_tokens if word not in string.punctuation]\n",
    "\n",
    "print(\"Final Tokens after Removing Stop Words and Punctuation:\\n\")\n",
    "print(final_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc106755",
   "metadata": {},
   "source": [
    "## 4. Explain the importance of filtering the stop words and punctuations in text analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d984b103",
   "metadata": {},
   "source": [
    "Filtering stop words and punctuations is a vital preprocesssing step in text analytics and natural language processing. The reasons include:\n",
    "\n",
    "1. Stop words carry minimal semantic value\n",
    "Words like 'the', 'is', 'in', and 'and' all fall under stop words, they frequently appear across texts but don't contribute to distinguishing between classes. Keeping stop words can obscure meaningful patterns in text.\n",
    "\n",
    "2. Reduces feature space dimensionality\n",
    "Remocing stop words reduces the number of tokens which simplifies the feature matrix and makes algorithms more efficient.\n",
    "\n",
    "3. Improves computational efficiency\n",
    "Machine learning models like Naive Bayes, Logistic Regression, or SVMs process fewer irrelevent features, which results in faster training and prediciton times which is critical in large datasets.\n",
    "\n",
    "4. Enhances model performance by reducing noise\n",
    "Stop words and punctuation introduces noise in statistical and machine learning model. In order for the model to focus on informative words, removing stop words and punctuation is necessary.\n",
    "\n",
    "5. Supports better semantic representation\n",
    "Removing irrelevant tokens allows for remaining words to better capture the essence of the text. This in return will improve downstream tasks such as sentiment analysis, spam detection or authorship attribution where meaningful patterns are subtle and context-dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45ce5f1",
   "metadata": {},
   "source": [
    "# INDIVIDUAL PART (Q5) - SALSABILA AURELIA OKTANIOPUTRI\n",
    "- Alternative approach implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9601cc1b",
   "metadata": {},
   "source": [
    "## 5.1 Alternative Approach Implementation : TD-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b28d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.24.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (2.4.1)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 26.0.1\n",
      "[notice] To update, run: C:\\Users\\asus\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# installing scikit-learnf for TD-IDF Vectorizer\n",
    "!pip install scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d1ceb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens using TF-IDF Vectorizer:\n",
      "\n",
      "['classification', 'is', 'the', 'task', 'of', 'choosing', 'correct', 'class', 'label', 'for', 'given', 'input', 'in', 'basic', 'tasks', 'each', 'considered', 'isolation', 'from', 'all', 'other', 'inputs', 'and', 'set', 'labels', 'defined', 'advance', 'has', 'number', 'interesting', 'variants', 'example', 'multiclass', 'instance', 'may', 'be', 'assigned', 'multiple', 'open', 'not', 'sequence', 'list', 'are', 'jointly', 'classified']\n"
     ]
    }
   ],
   "source": [
    "# Initialize TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Fit on your text (must be list format)\n",
    "tfidf.fit([text])\n",
    "\n",
    "# Extract vocabulary (this gives us the tokens)\n",
    "tfidf_tokens = list(tfidf.vocabulary_.keys())\n",
    "\n",
    "print(\"Tokens using TF-IDF Vectorizer:\\n\")\n",
    "print(tfidf_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "80bb0e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Split() ---\n",
      "Total tokens: 82\n",
      "Tokens with punctuation attached: 13\n",
      "Hyphenated words split incorrectly: 0\n",
      "Unique tokens: 55\n",
      "\n",
      "--- Regex ---\n",
      "Total tokens: 83\n",
      "Tokens with punctuation attached: 0\n",
      "Hyphenated words split incorrectly: 1\n",
      "Unique tokens: 50\n",
      "\n",
      "--- NLTK ---\n",
      "Total tokens: 94\n",
      "Tokens with punctuation attached: 13\n",
      "Hyphenated words split incorrectly: 0\n",
      "Unique tokens: 53\n",
      "\n",
      "--- TF-IDF ---\n",
      "Total tokens: 45\n",
      "Tokens with punctuation attached: 0\n",
      "Hyphenated words split incorrectly: 1\n",
      "Unique tokens: 45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def tokenizer_metrics(name, tokens, original_text):\n",
    "    total = len(tokens)\n",
    "    # Count tokens with punctuation attached (like input., tasks,)\n",
    "    punct_attached = sum(1 for t in tokens if any(p in t for p in string.punctuation))\n",
    "    # Unique tokens (vocab size)\n",
    "    unique = len(set(tokens))\n",
    "    \n",
    "    # Count hyphenated words in original text that were split incorrectly\n",
    "    hyphen_words = [word for word in original_text.split() if '-' in word]\n",
    "    hyphen_split_count = sum(1 for hw in hyphen_words if hw not in tokens)\n",
    "    \n",
    "    print(f\"--- {name} ---\")\n",
    "    print(\"Total tokens:\", total)\n",
    "    print(\"Tokens with punctuation attached:\", punct_attached)\n",
    "    print(\"Hyphenated words split incorrectly:\", hyphen_split_count)\n",
    "    print(\"Unique tokens:\", unique)\n",
    "    print()\n",
    "\n",
    "# Compare Tokenizers\n",
    "tokenizer_metrics(\"Split()\", split_tokens, text)\n",
    "tokenizer_metrics(\"Regex\", regex_tokens, text)\n",
    "tokenizer_metrics(\"NLTK\", nltk_tokens, text)\n",
    "tokenizer_metrics(\"TF-IDF\", tfidf_tokens, text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1c2c93",
   "metadata": {},
   "source": [
    "## 5.2 Compare and contrast the alternative approach with the group’s approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aeba49",
   "metadata": {},
   "source": [
    "The alternative approach implemented was using **TD-IDF** Vectorizers, which differs from the group's tokenization methods (Split(), Regular Expression, and NLTK) in both functionality and purpose. \n",
    "\n",
    "The group's approach focus on lexical tokenization, which means that they split text into tokens (wors) based on whitespace, patterns or linguistic rules. Method of tokenization differs for each approach, \n",
    "\n",
    "- Split() \n",
    "- Regular Expression \n",
    "- NLTK\n",
    "\n",
    "On the other hand, TF-IDF tokenization is integrated within machine learning feature extraction process. The process of how TF-IDF works is as follows:\n",
    "\n",
    "- Automatically converts text to lowercase\n",
    "- Removes punctuation\n",
    "- Splits hyphenated words\n",
    "- Produces a clean vocabulary suitable for modeling\n",
    "\n",
    "To conclude, **what differentiates TF-IDF with the group's approach is that it is designed for computational modeling and feature preparation**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67798827",
   "metadata": {},
   "source": [
    "## 5.3 Explain why the alternative approach is better, worse, or just different. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350b8295",
   "metadata": {},
   "source": [
    "The TF-IDF approach is different as it is not only a tokenizer but also is a part of a **feature extraction pipeline** that is designed for supervised learning task.\n",
    "\n",
    "**Reasons why TF-IDF approach can be better**:\n",
    "- It automatically normalizes text through lowercasing\n",
    "- Punctuations are removed which reduces noise\n",
    "- It oriduces clean & consistent tokens which are more suitable for classification models\n",
    "- It allows for the direct integration into machine learning workflows\n",
    "\n",
    "**Reasons why TF-IDF approach can be worse**:\n",
    "- It splits hyphenated words, which potentially alters semantic meaning\n",
    "- It does not automatically remove stopwords unless explicitly specified\n",
    "- It offers less linguistic control\n",
    "\n",
    "**To conclude**, classifying this approach to be better of worse is not possible because TF-IDF serves a different purpose. This approach is more suitable for machine learning-based text analytics because it directly prepares text for numerical representation and classification tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
