{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e39fa41f",
   "metadata": {},
   "source": [
    "# Part A (Q1) - SALSABILA AURELIA OKTANIOPUTRI\n",
    "- Tokenization, Stop Words & Punctuation \n",
    "- Dataset: Data_1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043d7f33",
   "metadata": {},
   "source": [
    "## Load Data_1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0b5698c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification is the task of choosing the correct class label for a given input. In basic\n",
      "classification tasks, each input is considered in isolation from all other inputs, and the set of labels is defined in advance. The basic classification task has a number of interesting variants. For example, in multiclass classification, each instance may be assigned multiple labels; in open-class classification, the set of labels is not defined in advance; and in sequence classification, a list of inputs are jointly classified.\n"
     ]
    }
   ],
   "source": [
    "# load the text file\n",
    "with open(\"../Data/Data_1.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1f08a6",
   "metadata": {},
   "source": [
    "## 1. Demonstrate word tokenization using the split function, Regular expression and NLTK packages separately and report the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4037b57f",
   "metadata": {},
   "source": [
    "### 1.1. Tokenization Using split()\n",
    "\n",
    "The split() function separates text based on whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22cdc0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens using split():\n",
      "\n",
      "['Classification', 'is', 'the', 'task', 'of', 'choosing', 'the', 'correct', 'class', 'label', 'for', 'a', 'given', 'input.', 'In', 'basic', 'classification', 'tasks,', 'each', 'input', 'is', 'considered', 'in', 'isolation', 'from', 'all', 'other', 'inputs,', 'and', 'the', 'set', 'of', 'labels', 'is', 'defined', 'in', 'advance.', 'The', 'basic', 'classification', 'task', 'has', 'a', 'number', 'of', 'interesting', 'variants.', 'For', 'example,', 'in', 'multiclass', 'classification,', 'each', 'instance', 'may', 'be', 'assigned', 'multiple', 'labels;', 'in', 'open-class', 'classification,', 'the', 'set', 'of', 'labels', 'is', 'not', 'defined', 'in', 'advance;', 'and', 'in', 'sequence', 'classification,', 'a', 'list', 'of', 'inputs', 'are', 'jointly', 'classified.']\n"
     ]
    }
   ],
   "source": [
    "split_tokens = text.split()\n",
    "print(\"Tokens using split():\\n\")\n",
    "print(split_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d9bfda",
   "metadata": {},
   "source": [
    "### 1.2. Tokenization Using Regular Expression\n",
    "RegEx extract only word characters and remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f39f2403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens using Regular Expression:\n",
      "\n",
      "['Classification', 'is', 'the', 'task', 'of', 'choosing', 'the', 'correct', 'class', 'label', 'for', 'a', 'given', 'input', 'In', 'basic', 'classification', 'tasks', 'each', 'input', 'is', 'considered', 'in', 'isolation', 'from', 'all', 'other', 'inputs', 'and', 'the', 'set', 'of', 'labels', 'is', 'defined', 'in', 'advance', 'The', 'basic', 'classification', 'task', 'has', 'a', 'number', 'of', 'interesting', 'variants', 'For', 'example', 'in', 'multiclass', 'classification', 'each', 'instance', 'may', 'be', 'assigned', 'multiple', 'labels', 'in', 'open', 'class', 'classification', 'the', 'set', 'of', 'labels', 'is', 'not', 'defined', 'in', 'advance', 'and', 'in', 'sequence', 'classification', 'a', 'list', 'of', 'inputs', 'are', 'jointly', 'classified']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "regex_tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "print(\"Tokens using Regular Expression:\\n\")\n",
    "print(regex_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b0e45a",
   "metadata": {},
   "source": [
    "### 1.3. Tokenization Using Regular Expression\n",
    "\n",
    "NLTK word_tokenize() handles punctuation more linguistically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f969d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe\n"
     ]
    }
   ],
   "source": [
    "# debugging\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69c5eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\asus/nltk_data', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\\\nltk_data', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\\\share\\\\nltk_data', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\asus\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    }
   ],
   "source": [
    "# debugging\n",
    "import nltk\n",
    "print(nltk.data.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02c3ea6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2fca952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens using NLTK: \n",
      "\n",
      "['Classification', 'is', 'the', 'task', 'of', 'choosing', 'the', 'correct', 'class', 'label', 'for', 'a', 'given', 'input', '.', 'In', 'basic', 'classification', 'tasks', ',', 'each', 'input', 'is', 'considered', 'in', 'isolation', 'from', 'all', 'other', 'inputs', ',', 'and', 'the', 'set', 'of', 'labels', 'is', 'defined', 'in', 'advance', '.', 'The', 'basic', 'classification', 'task', 'has', 'a', 'number', 'of', 'interesting', 'variants', '.', 'For', 'example', ',', 'in', 'multiclass', 'classification', ',', 'each', 'instance', 'may', 'be', 'assigned', 'multiple', 'labels', ';', 'in', 'open-class', 'classification', ',', 'the', 'set', 'of', 'labels', 'is', 'not', 'defined', 'in', 'advance', ';', 'and', 'in', 'sequence', 'classification', ',', 'a', 'list', 'of', 'inputs', 'are', 'jointly', 'classified', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk_tokens = word_tokenize(text)\n",
    "\n",
    "print(\"Tokens using NLTK: \\n\")\n",
    "print(nltk_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bf578b",
   "metadata": {},
   "source": [
    "### 2.\tJustify the most suitable tokenisation operation for text analytics. Support your answer using obtained outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbfeb332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Classification', 'is', 'the', 'task', 'of', 'choosing', 'the', 'correct', 'class', 'label', 'for', 'a', 'given', 'input.', 'In', 'basic', 'classification', 'tasks,', 'each', 'input', 'is', 'considered', 'in', 'isolation', 'from', 'all', 'other', 'inputs,', 'and', 'the', 'set', 'of', 'labels', 'is', 'defined', 'in', 'advance.', 'The', 'basic', 'classification', 'task', 'has', 'a', 'number', 'of', 'interesting', 'variants.', 'For', 'example,', 'in', 'multiclass', 'classification,', 'each', 'instance', 'may', 'be', 'assigned', 'multiple', 'labels;', 'in', 'open-class', 'classification,', 'the', 'set', 'of', 'labels', 'is', 'not', 'defined', 'in', 'advance;', 'and', 'in', 'sequence', 'classification,', 'a', 'list', 'of', 'inputs', 'are', 'jointly', 'classified.']\n"
     ]
    }
   ],
   "source": [
    "print(split_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2619d5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Split() ---\n",
      "Total tokens: 82\n",
      "Tokens with punctuation attached: 13\n",
      "Hyphenated words split incorrectly: 0\n",
      "Unique tokens: 55\n",
      "\n",
      "--- Regex ---\n",
      "Total tokens: 83\n",
      "Tokens with punctuation attached: 0\n",
      "Hyphenated words split incorrectly: 1\n",
      "Unique tokens: 50\n",
      "\n",
      "--- NLTK ---\n",
      "Total tokens: 94\n",
      "Tokens with punctuation attached: 13\n",
      "Hyphenated words split incorrectly: 0\n",
      "Unique tokens: 53\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def tokenizer_metrics(name, tokens, original_text):\n",
    "    total = len(tokens)\n",
    "    # Count tokens with punctuation attached (like input., tasks,)\n",
    "    punct_attached = sum(1 for t in tokens if any(p in t for p in string.punctuation))\n",
    "    # Unique tokens (vocab size)\n",
    "    unique = len(set(tokens))\n",
    "    \n",
    "    # Count hyphenated words in original text that were split incorrectly\n",
    "    hyphen_words = [word for word in original_text.split() if '-' in word]\n",
    "    hyphen_split_count = sum(1 for hw in hyphen_words if hw not in tokens)\n",
    "    \n",
    "    print(f\"--- {name} ---\")\n",
    "    print(\"Total tokens:\", total)\n",
    "    print(\"Tokens with punctuation attached:\", punct_attached)\n",
    "    print(\"Hyphenated words split incorrectly:\", hyphen_split_count)\n",
    "    print(\"Unique tokens:\", unique)\n",
    "    print()\n",
    "\n",
    "# Compare Tokenizers\n",
    "tokenizer_metrics(\"Split()\", split_tokens, text)\n",
    "tokenizer_metrics(\"Regex\", regex_tokens, text)\n",
    "tokenizer_metrics(\"NLTK\", nltk_tokens, text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4018d4a0",
   "metadata": {},
   "source": [
    "Three tokenization methods are evaluated: **Split()**, **Regular Expression (Regex)**, and **NLTK's `word_tokenize()`**.  \n",
    "The evaluation metrics are:  \n",
    "\n",
    "- **Total Tokens** – number of tokens generated  \n",
    "- **Tokens with Punctuation Attached** – words with punctuation still stuck (`input.`, `tasks,`)  \n",
    "- **Unique Tokens** – vocabulary size  \n",
    "- **Hyphenated Words Split Incorrectly** – counts hyphenated words like `open-class` that got split incorrectly  \n",
    "\n",
    "**1. split()**\n",
    "Metrics from the results: \n",
    "  - Total tokens: 82  \n",
    "  - Tokens w/ punctuation attached: 13  \n",
    "  - Unique tokens: 55  \n",
    "  - Hyphenated words split incorrectly: 0  \n",
    "\n",
    "Analysis & Proof:\n",
    "- Punctuation is not separated; 13 tokens have punctuation still attached. Example: `input.` and `tasks,`.  \n",
    "- Hyphenated words are preserved because punctuation stays attached (`open-class` counted as one token).  \n",
    "- Limitation: Punctuation attached inflates vocabulary and creates noise for ML models.\n",
    "\n",
    "\n",
    "**2. Regular Expression**\n",
    "Metrics from the results:\n",
    "  - Total tokens: 83  \n",
    "  - Tokens w/ punctuation attached: 0  \n",
    "  - Unique tokens: 50  \n",
    "  - Hyphenated words split incorrectly: 1  \n",
    "\n",
    "Analysis & Proof:\n",
    "- Punctuation is completely removed, 0 punctuation-attached tokens.  \n",
    "- Example: `input.` → `input`, `tasks,` → `tasks`.  \n",
    "- Problem: Hyphenated word `open-class` is split into `open` and `class` → semantic meaning lost.  \n",
    "- Vocabulary slightly smaller (50 unique tokens) due to punctuation removal.  \n",
    "\n",
    "**3. NLTK**\n",
    "Metrics from the results:\n",
    "  - Total tokens: 94  \n",
    "  - Tokens w/ punctuation attached: 13  \n",
    "  - Unique tokens: 53  \n",
    "  - Hyphenated words split incorrectly: 0  \n",
    "\n",
    "Analysis & Proof:\n",
    "- Punctuation is separated as individual tokens. Example from first 15 tokens: ['input', '.', 'tasks', ',', 'labels', ';', 'open-class']\n",
    "- the period (`.`), comma (`,`), and semicolon (`;`) are counted as separate tokens.  \n",
    "- Hyphenated words (`open-class`) are preserved → correct semantic representation.  \n",
    "- Total token count is higher because punctuation is separated, but this allows controlled preprocessing (e.g., removing punctuation or stopwords).  \n",
    "- Vocabulary size (53 unique tokens) is appropriate — balances completeness and accuracy.\n",
    "\n",
    "**Conclusion**\n",
    "- **Split()**: Keeps punctuation attached however iw will inflates tokens which is not ideal for ML preprocessing.  \n",
    "- **Regex**: Clean punctuation which is good for counting, but splits hyphenated words thus loses semantic meaning.  \n",
    "- **NLTK**: Linguistically aware but separates punctuation, preserves hyphenated words. NLTK is the  **best choice** for downstream text analytics tasks.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55364f9e",
   "metadata": {},
   "source": [
    "answer it here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6374ea7f",
   "metadata": {},
   "source": [
    "### 3.\tDemonstrate stop words and punctuations removal and report the output suitably along with the stop words found in the given text corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca176d1",
   "metadata": {},
   "source": [
    "### 3.1. Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb6122af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Words Found in Corpus:\n",
      "\n",
      "{'and', 'each', 'in', 'has', 'be', 'is', 'the', 'not', 'are', 'all', 'from', 'a', 'of', 'other', 'for'}\n",
      "\n",
      "Tokens after Stop Word Removal:\n",
      "\n",
      "['classification', 'task', 'choosing', 'correct', 'class', 'label', 'given', 'input', '.', 'basic', 'classification', 'tasks', ',', 'input', 'considered', 'isolation', 'inputs', ',', 'set', 'labels', 'defined', 'advance', '.', 'basic', 'classification', 'task', 'number', 'interesting', 'variants', '.', 'example', ',', 'multiclass', 'classification', ',', 'instance', 'may', 'assigned', 'multiple', 'labels', ';', 'open-class', 'classification', ',', 'set', 'labels', 'defined', 'advance', ';', 'sequence', 'classification', ',', 'list', 'inputs', 'jointly', 'classified', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Lowercase tokens\n",
    "tokens_lower = [word.lower() for word in nltk_tokens]\n",
    "\n",
    "# Remove stopwords\n",
    "filtered_tokens = [word for word in tokens_lower if word not in stop_words]\n",
    "\n",
    "print(\"Stop Words Found in Corpus:\\n\")\n",
    "found_stopwords = [word for word in tokens_lower if word in stop_words]\n",
    "print(set(found_stopwords))\n",
    "\n",
    "print(\"\\nTokens after Stop Word Removal:\\n\")\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ce224e",
   "metadata": {},
   "source": [
    "### 3.2. Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89fc1c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Tokens after Removing Stop Words and Punctuation:\n",
      "\n",
      "['classification', 'task', 'choosing', 'correct', 'class', 'label', 'given', 'input', 'basic', 'classification', 'tasks', 'input', 'considered', 'isolation', 'inputs', 'set', 'labels', 'defined', 'advance', 'basic', 'classification', 'task', 'number', 'interesting', 'variants', 'example', 'multiclass', 'classification', 'instance', 'may', 'assigned', 'multiple', 'labels', 'open-class', 'classification', 'set', 'labels', 'defined', 'advance', 'sequence', 'classification', 'list', 'inputs', 'jointly', 'classified']\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "final_tokens = [word for word in filtered_tokens if word not in string.punctuation]\n",
    "\n",
    "print(\"Final Tokens after Removing Stop Words and Punctuation:\\n\")\n",
    "print(final_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc106755",
   "metadata": {},
   "source": [
    "## 4. Explain the importance of filtering the stop words and punctuations in text analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d984b103",
   "metadata": {},
   "source": [
    "The reason why filtering stop words and punctuation is important are as follow:\n",
    "\n",
    "1. Stop words such as \"the\", \"is\", and \"in\" do not carry significant semantic meaning.\n",
    "2. Removing them reduces dimensionality of the feature space.\n",
    "3. It improves computational efficiency.\n",
    "4. It enhances model performance by reducing noise.\n",
    "5. Punctuation does not contribute to classification tasks and may distort word frequency analysis.\n",
    "\n",
    "Therefore, filtering improves the quality of text representation in text analytics tasks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
